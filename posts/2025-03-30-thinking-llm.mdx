---
title: '关于AI（LLM）'
date: '2025-03-29T19:38:15.248Z'
tags: 
 - LLM
abstract: 哈哈 是对 AI 的一些看法
---

现在是 25 年 3 月 30 日（emmm 博客似乎暂时没有支持 Time 组件）。

最近在面试中经常会遇到这个问题，每次听到这个问题我就想到一些场景：

- 22 年 11 月在一节矩阵的课上，我拿着 iPad，给同学展示一个可以输出代码的东西。
- 22 年年中，在家看李宏毅老师的课程，讲到一种奇怪的问答训练方式，为了做一个全能的模型。
- 22 年年中，看完 Transformer 和 BERT 的论文，以及 BERT 附录里的连接线。

是的，这些都是了解 GPT（LLM）类模型的一些方式，第一个是看到它的应用，第二个是观察了它的训练，第三个则是看到了它的完形填空。那这篇 blog，只是想聊一聊，这些年，哪些东西发生了**变化**，哪些东西**未变**，以及**未来**。另外，这些都是一些非常非常浅显的理解，因为很关键的是，我没有从零训练 LLM 的经验。如果是从业者，他们一定非常清除这些痛点。

## 变化

> 不真实和不可信、不稳定

这是我在了解它的原理，以及刚接触到的最大感受。因为本质上对于这类生成模型，它只是服从某个分布罢了。如果这个分布一开始就错了，那就会错的没边，最典型的就是让它做加法，在 22 年的时候，根本就算不对。对于一个只会玩接龙的家伙，你想让它懂逻辑，那恐怕有点困难。再或者让它去说一些新闻相关的问题，或者推荐一些什么，一开始几个是对的，后面就开始了各种编造。所以最直观的感受就来了，**不真实**，那自然，也**不可信**。这在现在被归纳为所谓的幻觉，其实这个多半是用于训练资料不够充分所导致的。

问其同样的问题，得到的答案是不一样的。因为对于它来说，需要 Seed（随机种子）和温度，以及采样策略控制其生成**下一个字**的概率。使用随机或调整温度的主要目的是为了保证模型具备一定的创造能力，让其对于下一个字的预测**有一定**的随机性。这里的**有一定**，指的是由温度调整的分布扩散范围，在训练时，期望这个随机性的范围是一个较为“正确”的结果。至于为什么要具备创造力，我想应该是不希望这个模型本身只是一个 switch 罢了，也就是我们期望对于同样的问题有多种解法。这样的设计过程，会产生一个矛盾点，也是你在 coding 时最讨厌的东西，就是不确定性，带给我的直观感受就是**不稳定**。这在 23 年年初的时候异常明显，我们可能需要

> 多试几次

才能得到**我们认为**的正确结果（分布）。

你会注意到，我在上面刻意的强调了时间点。是的，这三个点在从那会到现在（25年年初）都发生了一些转变。

首先是不真实和不可信。我对这个发生较大转变的感觉是早期使用的 pplx（使用了搜索引擎和爬虫，这里我要稍微吐槽一下 Bing，明明可以很早的把这个做好，但实际上是最烂的）以及现在的 Grok 3，其中 pplx 借助了外部 **Context**，也就是所谓的知识库去尝试消除一些幻觉，寄希望于模型**自己去查询**结果并总结出答案，而 Grok 3 则是拥有一个较大的训练集，同时也是基于社交网络（X/Twitter/Reddit）训练出来带来的优势，他可能在我所处的领域（CS）有着不错的知识面，因为这些地方也是这种群体讨论最多的地方。可能对于以前的我来说，我会不太信任 LLM 的结论，大概率是错误的。那么现在，虽然仍持怀疑态度，但不会像原来一样直接考虑搜索引擎，而是先尝试从 LLM 这里获得相关内容后，再去阅读其引用的文档。所以工作流发生的变化，意味着我对它信任的上升。

其次是不稳定。程序员喜欢稳定的东西，因为代码得是健壮的。特别是用 React 的家伙，希望函数都是纯的，而不是因为某些奇怪的东西，导致跑两次。那 LLM 这种不稳定，就会给人一种抵触的感觉，我期望它每次都能做正确的事，而实际上只是有概率会，这会让我提心吊胆。这和前面的幻觉是不一样的，如果一个问题，它出现了幻觉，那实际上你无论怎么问，只要意思相近，几乎都会出现幻觉。不稳定则在强调其能够给出正确的结果，但有时会是错误的。为了尽可能的缓解这个问题，催生出了所谓的 Prompts 工程学，旨在以一个标准化的 Prompts 令尽可能的落在期望的分布里。而这个问题，在引入了所谓的“多轮计算“也就是”深度思考“之后得到了极大的缓解，因为这是在期望模型按步骤做事，先将问题拆分，再去解决问题的思路去模拟人类的思考过程，随后再根据这种”思考过程“（初次生成结果）去生成最终结果。这是有代价的，这是缓慢的。因为需要多次的轮回，甚至会导致无限循环，而错失了自动跳出的机会。但再一定程度上缓解了这种所谓的不稳定。

## 未变

> 你的 LLM 终究是概率和 Transformer，而思考过程，也是对人类的拙劣模仿。搜索过程也只是外接背景板，还会受到上下文的限制。

我不会对 LLM 做推理这件事抱有太大的期望，而实际上现在所谓做推理的能力也只是预设好的一部分。它终究是一个概率模型，这是从未改变的事实。因此我对它的期望也是一个遵循概率而去工作的机器，而非人工智能。最典型的就是约束力，人类可以约束自己不去做一些事，而 LLM 并没有那么好控制，概率成就了它，也毁了它。

> 人工智能，不应该是这样的形态。

因此我在标题里有提到，这是 LLM，而非 AI。

> 它在做的工作始终只是 Retrieval，这只能成为一个 Retrieval 工具。

这非常的爆论对吧，LLM 是无法“理解”的，它只是在一味的计算新的概率，去**匹配**下一个词，去用一种模式（参数）去识别（计算）下一个词罢了。也就是说，我们接触到的并非人工智能，而是一个**工具**而已，这个工具是非常冰冷的搜索引擎。

## 未来

既然是一个工具，那我就可以利用它去做一些它擅长东西。而现在所谓 “AI” 的发展，也在推崇这种工具的使用，来替代掉一些可以通过 Retrieval 达成的事情。最典型的就是知识库检索、Coding 和总结。

我更期望看到的是一个概率模型之外的东西，我无法想象那是一个什么样的东西。

另外，我发现我会对 LLM 生成的东西抱有一种额外的反感，特别是在论坛或者社交网络上看到。由于其训练过程导致其有一些固定的范式，让人有种很不舒服的感觉。同时最近流行的 GPT-4o 生成图片也是，我对 sd 生成的图也有类似的感觉。因此，我也希望能够为所有由所谓的 “AI” 生成的东西做上标记，做上记号，特别是在互联网传播的时候，以减少误导性。同样，对于代码，我也期望有类似的注释，让我们对其有个基本的预期，希望之后能够标准化吧。因为我想看到的是更多**有感情、有意识**的人类创造的东西，而不是基于某种“模式”，“识别”出来的东西，加以区分是最低的要求。

如果你在尝试使用 LLM 生成的东西作为存档的产物，我期望你能保持谨慎的去使用。生成的结果不应该成为一种 “context”，而更应该作为一种“检索结果”，这样的临时概念。所有需要存档的东西，应该仍然由人类来完成，尤其是文档，只有这样，才能保持住最后的可控性。

**这篇文章中不存在任何由 LLM 生成的内容，也未经 LLM 润色。**
